{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File #2\n",
    "# Trains the classifier and returns the predicted flare for the \n",
    "# single URL as well as for automated_testing\n",
    "\n",
    "# pip install praw scikit-learn pandas\n",
    "import requests\n",
    "import praw\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads the CSV and trains a text classifier\n",
    "# using Bag Of Words technique.\n",
    "# Saves the model to text_clf.pkl\n",
    "\n",
    "def build_classifier():\n",
    "\n",
    "    data = pd.read_csv('scraped_reddit.csv')\n",
    "\n",
    "    X = data[\"title\"]\n",
    "    y = data[\"flair\"]\n",
    "    #Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Using CountVect for BOW model\n",
    "    # TFIDF: reduce the weightage of more common words\n",
    "    # SVM as classifier\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b')),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=1e-3, random_state=42)),\n",
    "    ])\n",
    "    text_clf = text_clf.fit(X_train.values.astype('U'), y_train.values.astype('U'))\n",
    "\n",
    "    #Save the model\n",
    "    joblib.dump(text_clf, 'text_clf.pkl') \n",
    "    \n",
    "    # For testing Purpose\n",
    "    #predicted = text_clf.predict(X_test.values.astype('U'))\n",
    "    #print(numpy.mean(predicted == y_test.values.astype('U')))\n",
    "    #print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the data for the specified reddit URL\n",
    "# And then returns the predicted Flair\n",
    "\n",
    "def calculate_flare(input):\n",
    "    topics_dict = { \"title\":[], \n",
    "                \"score\":[], \n",
    "                \"id\":[], \"url\":[],  \n",
    "                \"comms_num\": [], \n",
    "                \"created\": [], \n",
    "                \"body\":[],\n",
    "                \"flair\":[]}\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission/?url=\" + input\n",
    "    json = requests.get(url, headers={'User-Agent': \"utsavgoel\"})\n",
    "    json_data = json.json()\n",
    "    #print(json_data['data'][0])\n",
    "    if 'data' not in json_data or len(json_data['data']) == 0:\n",
    "        print(\"No Data Fetched\")\n",
    "        return\n",
    "    object = json_data['data'][0]\n",
    "    if 'link_flair_text' in object:\n",
    "        topics_dict[\"title\"].append(object['title'])\n",
    "        topics_dict[\"score\"].append(object['score'])\n",
    "        topics_dict[\"id\"].append(object['id'])\n",
    "        topics_dict[\"url\"].append(object['url'])\n",
    "        topics_dict[\"comms_num\"].append(object['num_comments'])\n",
    "        topics_dict[\"created\"].append(object['created_utc'])\n",
    "        topics_dict[\"body\"].append(object['selftext'])\n",
    "        topics_dict[\"flair\"].append(object['link_flair_text']) \n",
    "    else:\n",
    "        print(\"No flair in the Reddit Object\")\n",
    "          \n",
    "    text_clf = joblib.load('text_clf.pkl')  \n",
    "    predicted = text_clf.predict(topics_dict[\"title\"])\n",
    "    print(predicted)\n",
    "    return {'url':input, 'flare':predicted[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the .txt file and returns predicted flare\n",
    "# in a dictionary\n",
    "def testing(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as fin:\n",
    "        for line in fin:\n",
    "            data.append(calculate_flare(line))\n",
    "\n",
    "    with open('data.json', 'w') as fout:\n",
    "        json.dump(data, fout)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Policy/Economy']\n",
      "['Policy/Economy']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if os.path.exists('text_clf.pkl'):\n",
    "        build_classifier()\n",
    "    else:\n",
    "        pass\n",
    "    #testing(\"abc.txt\")  \n",
    "    #input = \"https://www.reddit.com/r/india/comments/g1zi21/coronavirus_covid19_megathread_news_and_updates_4/\"\n",
    "    #calculate_flare(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
